{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f3946c-63b2-4281-a321-c7f5ffe9f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/viniciuss/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/viniciuss/.local/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: sklearn2pmml in /home/viniciuss/.local/lib/python3.10/site-packages (0.109.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/viniciuss/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/viniciuss/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/viniciuss/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/viniciuss/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/viniciuss/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/viniciuss/.local/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/viniciuss/.local/lib/python3.10/site-packages (from sklearn2pmml) (0.3.8)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn sklearn2pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d58d88f9-52d8-406a-bdf3-4cb41fedc2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from sklearn2pmml import PMMLPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fd8c262-c482-4398-ab7a-e66cfe1d6aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           landmarks  expression\n",
      "0  [0.5326159596443176, 0.7009046077728271, 0.567...           2\n",
      "1  [0.49526625871658325, 0.6726012229919434, 0.45...           2\n",
      "2  [0.5196071267127991, 0.7114415168762207, 0.512...           2\n",
      "3  [0.5070357918739319, 0.7234506011009216, 0.497...           2\n",
      "4  [0.5144729018211365, 0.7218216061592102, 0.515...           2\n",
      "expression\n",
      "2    14000\n",
      "1    14000\n",
      "3    14000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_209849/1149560225.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anger_half['expression'] = anger_half['expression'].replace(6, 2)\n",
      "/tmp/ipykernel_209849/1149560225.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sadness_half['expression'] = sadness_half['expression'].replace(6, 2)\n"
     ]
    }
   ],
   "source": [
    "# Ler os arquivos Parquet\n",
    "anger = pd.read_parquet('/home/viniciuss/Documents/projetos/emotion-recognition-ml/data/landmarks_data_manually_anotated_ANGER_14mil_samples.parquet')\n",
    "happiness = pd.read_parquet('/home/viniciuss/Documents/projetos/emotion-recognition-ml/data/landmarks_data_manually_anotated_HAPPINESS_14mil_samples.parquet')\n",
    "sadness = pd.read_parquet('/home/viniciuss/Documents/projetos/emotion-recognition-ml/data/landmarks_data_manually_anotated_SADNESS_14mil_samples.parquet')\n",
    "surprise = pd.read_parquet('/home/viniciuss/Documents/projetos/emotion-recognition-ml/data/landmarks_data_manually_anotated_SURPRISE_14mil_samples.parquet')\n",
    "\n",
    "# Dividir os dados ao meio\n",
    "half_sadness = len(sadness) // 2\n",
    "half_anger = len(anger) // 2\n",
    "\n",
    "sadness_half = sadness.iloc[:half_sadness]\n",
    "anger_half = anger.iloc[:half_anger]\n",
    "\n",
    "# Atualizar os valores de 'expression' diretamente\n",
    "anger_half['expression'] = anger_half['expression'].replace(6, 2)\n",
    "sadness_half['expression'] = sadness_half['expression'].replace(6, 2)\n",
    "happiness['expression'] = happiness['expression'].replace(6, 2)\n",
    "surprise['expression'] = surprise['expression'].replace(6, 2)\n",
    "\n",
    "# Combinar as metades em um novo DataFrame\n",
    "sadness_anger_combined = pd.concat([sadness_half, anger_half], ignore_index=True)\n",
    "\n",
    "# Juntar todos os DataFrames em um único DataFrame\n",
    "data = pd.concat([sadness_anger_combined, happiness, surprise], ignore_index=True)\n",
    "\n",
    "# Verificar as primeiras linhas e as contagens de cada emoção\n",
    "print(data.head())\n",
    "print(data['expression'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fe708a12-5f14-4bfb-9783-73dbdb8ad07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificar o Conteúdo de landmarks:\n",
      "0    [0.5326159596443176, 0.7009046077728271, 0.567...\n",
      "1    [0.49526625871658325, 0.6726012229919434, 0.45...\n",
      "2    [0.5196071267127991, 0.7114415168762207, 0.512...\n",
      "3    [0.5070357918739319, 0.7234506011009216, 0.497...\n",
      "4    [0.5144729018211365, 0.7218216061592102, 0.515...\n",
      "Name: landmarks, dtype: object\n",
      "Confirmar se landmarks Contém Dados:\n",
      "landmarks\n",
      "False    42000\n",
      "Name: count, dtype: int64\n",
      "Verificar alguns elementos de landmarks para entender o formato\n",
      "landmarks\n",
      "<class 'numpy.ndarray'>    41621\n",
      "<class 'NoneType'>           379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Verificar o Conteúdo de landmarks:')\n",
    "print(data['landmarks'].head())\n",
    "print('Confirmar se landmarks Contém Dados:')\n",
    "print(data['landmarks'].apply(lambda x: isinstance(x, list)).value_counts())\n",
    "print('Verificar alguns elementos de landmarks para entender o formato')\n",
    "print(data['landmarks'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5c0933d-f891-4d39-9304-0b5eb9295ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes dos landmarks:\n",
      "landmarks\n",
      "(956,)    41621\n",
      "Name: count, dtype: int64\n",
      "Comprimentos dos arrays de landmarks:\n",
      "landmarks\n",
      "956    41621\n",
      "Name: count, dtype: int64\n",
      "Exemplo de alguns dados em landmarks após limpeza:\n",
      "0    [0.5326159596443176, 0.7009046077728271, 0.567...\n",
      "1    [0.49526625871658325, 0.6726012229919434, 0.45...\n",
      "2    [0.5196071267127991, 0.7114415168762207, 0.512...\n",
      "3    [0.5070357918739319, 0.7234506011009216, 0.497...\n",
      "4    [0.5144729018211365, 0.7218216061592102, 0.515...\n",
      "5    [0.4272332489490509, 0.7639281749725342, 0.423...\n",
      "6    [0.4863489270210266, 0.6434009671211243, 0.487...\n",
      "7    [0.46001899242401123, 0.732892632484436, 0.452...\n",
      "8    [0.54392009973526, 0.7221582531929016, 0.52595...\n",
      "9    [0.4205576777458191, 0.8216860294342041, 0.447...\n",
      "Name: landmarks, dtype: object\n",
      "(41621, 956)\n",
      "Conteúdo de alguns exemplos de landmarks:\n",
      "0    [0.5326159596443176, 0.7009046077728271, 0.567...\n",
      "1    [0.49526625871658325, 0.6726012229919434, 0.45...\n",
      "2    [0.5196071267127991, 0.7114415168762207, 0.512...\n",
      "3    [0.5070357918739319, 0.7234506011009216, 0.497...\n",
      "4    [0.5144729018211365, 0.7218216061592102, 0.515...\n",
      "Name: landmarks, dtype: object\n",
      "Verificar o Número de Linhas Após Remoção de NaNs:\n",
      "Número de linhas antes da remoção de NaNs: 41621\n",
      "Número de linhas após a remoção de NaNs: 41621\n",
      "Verificar a Forma de X e data:\n",
      "X shape: (41621, 956)\n",
      "Número de NaNs em X: 0\n"
     ]
    }
   ],
   "source": [
    "def convert_landmarks_to_array(df):\n",
    "    # Substituir NoneType por NaN\n",
    "    df['landmarks'] = df['landmarks'].apply(lambda x: np.nan if x is None else x)\n",
    "    \n",
    "    # Verificar formas antes da conversão\n",
    "    check_landmarks_shapes(df)\n",
    "    \n",
    "    # Converter landmarks para arrays numpy\n",
    "    landmarks = df['landmarks'].apply(lambda x: np.array(x) if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "    \n",
    "    # Remover linhas com valores NaN\n",
    "    df = df.dropna(subset=['landmarks'])\n",
    "    \n",
    "    # Verificar e filtrar arrays com comprimento inesperado\n",
    "    valid_lengths = df['landmarks'].apply(lambda x: len(x) if isinstance(x, np.ndarray) else None).value_counts()\n",
    "    print('Comprimentos dos arrays de landmarks:')\n",
    "    print(valid_lengths)\n",
    "    \n",
    "    expected_length = valid_lengths.idxmax()  # Usar o comprimento mais frequente\n",
    "    df = df[df['landmarks'].apply(lambda x: len(x) == expected_length if isinstance(x, np.ndarray) else False)]\n",
    "    \n",
    "    # Exibir exemplos de dados para verificar inconsistências\n",
    "    print('Exemplo de alguns dados em landmarks após limpeza:')\n",
    "    print(df['landmarks'].head(10))\n",
    "    \n",
    "    # Converter listas para arrays numpy\n",
    "    try:\n",
    "        X = np.array([np.array(x) for x in df['landmarks']])\n",
    "    except ValueError as e:\n",
    "        print(\"Erro ao converter landmarks para array numpy:\", e)\n",
    "        print(\"Exemplo de alguns dados em landmarks:\")\n",
    "        print(df['landmarks'].head(10))\n",
    "        raise\n",
    "\n",
    "    # Verificar se X é unidimensional e ajustar se necessário\n",
    "    if X.ndim == 1:\n",
    "        # Se X for unidimensional, converte para 2D\n",
    "        X = np.array([x.flatten() for x in X])\n",
    "    \n",
    "    # Remove qualquer linha com NaN em X\n",
    "    if np.any(np.isnan(X)):\n",
    "        valid_indices = ~np.any(np.isnan(X), axis=1)\n",
    "        X = X[valid_indices]\n",
    "        df = df.iloc[valid_indices]\n",
    "    \n",
    "    return df, X\n",
    "\n",
    "# Testar a função\n",
    "data, X = convert_landmarks_to_array(data)\n",
    "y = data['expression']\n",
    "\n",
    "# Exibir resultados\n",
    "print(X.shape)\n",
    "print('Conteúdo de alguns exemplos de landmarks:')\n",
    "print(data['landmarks'].head())\n",
    "print('Verificar o Número de Linhas Após Remoção de NaNs:')\n",
    "print(\"Número de linhas antes da remoção de NaNs:\", len(data))\n",
    "print(\"Número de linhas após a remoção de NaNs:\", len(data))  # Corrigido para usar 'data' em vez de 'df'\n",
    "print('Verificar a Forma de X e data:')\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Número de NaNs em X:\", np.isnan(X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1fa6eae8-70bf-4b04-bcf1-3b1c3805bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (33296, 956)\n",
      "X_test shape: (8325, 956)\n",
      "y_train shape: (33296,)\n",
      "y_test shape: (8325,)\n"
     ]
    }
   ],
   "source": [
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Exibir as dimensões dos conjuntos de dados\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fd053-7391-4dc2-982a-4b492752547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o pipeline com os parâmetros especificados\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(\n",
    "        probability=False,\n",
    "        cache_size=500,\n",
    "        C=50,\n",
    "        class_weight='balanced',\n",
    "        coef0=0.0,\n",
    "        degree=2,\n",
    "        gamma=1.0,\n",
    "        kernel='poly'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Treinar o pipeline com os dados de treino\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad97df-2b51-407a-a385-c77ac73dd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Fazer previsões usando o pipeline treinado\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calcular a acurácia das previsões\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Acurácia: {accuracy:.2f}\")\n",
    "\n",
    "# Matriz de confusão\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=pipeline.classes_, yticklabels=pipeline.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c33154e-85f3-42c5-af68-48bc31625307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard output is empty\n",
      "Standard error:\n",
      "Exception in thread \"main\" org.jpmml.python.AttributeException: Attribute 'sklearn.svm._classes.SVC.classes_' not set\n",
      "\tat org.jpmml.python.PythonObject.getattr(PythonObject.java:114)\n",
      "\tat org.jpmml.python.PythonObject.get(PythonObject.java:142)\n",
      "\tat org.jpmml.python.PythonObject.getObject(PythonObject.java:194)\n",
      "\tat org.jpmml.python.PythonObject.getListLike(PythonObject.java:510)\n",
      "\tat sklearn.Classifier.getClasses(Classifier.java:93)\n",
      "\tat sklearn.Classifier.getClasses(Classifier.java:89)\n",
      "\tat sklearn.Classifier.encodeLabel(Classifier.java:118)\n",
      "\tat org.jpmml.sklearn.SkLearnEncoder.initLabel(SkLearnEncoder.java:149)\n",
      "\tat sklearn.Composite.initLabel(Composite.java:226)\n",
      "\tat sklearn.pipeline.SkLearnPipeline.encodePMML(SkLearnPipeline.java:226)\n",
      "\tat com.sklearn2pmml.Main.run(Main.java:80)\n",
      "\tat com.sklearn2pmml.Main.main(Main.java:65)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The SkLearn2PMML application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Salvar PMML\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pmml_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/viniciuss/Documents/projetos/emotion-recognition-api/src/main/resources/model/emotion_recognition_model.pmml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msklearn2pmml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_repr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn2pmml/__init__.py:335\u001b[0m, in \u001b[0;36msklearn2pmml\u001b[0;34m(estimator, pmml_path, with_repr, java_home, java_opts, user_classpath, dump_flavour, debug)\u001b[0m\n\u001b[1;32m    333\u001b[0m \t\t\t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard error is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[0;32m--> 335\u001b[0m \t\t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe SkLearn2PMML application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    337\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The SkLearn2PMML application has failed. The Java executable should have printed more information about the failure into its standard output and/or standard error streams"
     ]
    }
   ],
   "source": [
    "# Salvar PMML\n",
    "pmml_path = \"/home/viniciuss/Documents/projetos/emotion-recognition-api/src/main/resources/model/emotion_recognition_model.pmml\"\n",
    "sklearn2pmml(pipeline, pmml_path, with_repr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40816-ef14-4048-97c7-c2c94ed345f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
